---
title: "Exploratory Analysis: Presidential Voting by County"
output: html_notebook
---


# INTRODUCTION

This notebook is an accompaniment to app.R, a Shiny app that generates decision tree models
based on demographic and election data. Much of the code in this notebook is replicated
in app.R, but I've included some exploratory data analysis and lengthier comments here.

We're using data from a reasonably wide variety of sources, some local and some accessed through APIs. 

# LOADING LIBRARIES

```{r}
library(shiny) 
library(tidyverse)
library(RCurl) 
library(rpart)
library(maps)
library(mapproj)
library(caret)
library(rpart.plot)
```

# LOADING DATA

In this section we load all of the data we're using for this project into tidy tibbles. Eventually these will be joined together into one large tibble (called df) which will be narrowed down to a smaller tibble called main_df.


Get 2010 US census data (data on county-level racial composition, as well as population). This comes from a local file that should be available in the data directory of the Github repository. 

```{r load_census}
census_df <- readRDS("data/census.rds")
census_df <- as_tibble(census_df)
census_df <- census_df %>%
    rename(total_pop = total.pop)
```

Get population density data. This was calculated by Github user ykzeng using census data and Wikipedia.

#TODO: Find more authoritative source for pop. density data.

```{r load_density}

density_url <- getURL("https://raw.githubusercontent.com/ykzeng/covid-19/master/data/census-population-landarea.csv")
density_df <- read_csv(density_url)

```

Get USDA county-level education statistics. (This is the first of a handful of county-level data sets hosted by the USDA at https://www.ers.usda.gov/data-products/county-level-data-sets/)

```{r load_educ}

educ_url <- getURL("https://www.ers.usda.gov/webdocs/DataFiles/48747/Education.csv")
educ_df <- read_csv(educ_url)

```

Get USDA poverty stats: 

```{r load_pov}

pov_url <- getURL("https://www.ers.usda.gov/webdocs/DataFiles/48747/PovertyEstimates.csv")
pov_df <- read_csv(pov_url)

```

Get USDA unemployment stats:

```{r load_unemp}

emp_url <-  getURL("https://www.ers.usda.gov/webdocs/DataFiles/48747/Unemployment.csv")
emp_df <- read_csv(emp_url)

```

Get USDA pop stats: 

```{r load_pop}

pop_url <- getURL("https://www.ers.usda.gov/webdocs/DataFiles/48747/PopulationEstimates.csv")
pop_df <- read_csv(pop_url)

```

There is a problem here in the RESIDUAL_2010 column, but as we will not be using that column we ignore it.

The last data set we want is a .csv containing countywide election data. This ultimately comes from the NYT, which has it in a somewhat complicated .json file on their Github repository. 

We figured out how to get this for a previous project and the code to generate it is contained in the script munging.py. (We don't need to run the Python script for this project, but we include it for completeness sake.)

```{r load_pres}

vote_df <- read_csv('data/pres_race_2020.csv')

```



# INSPECTING DATA

It's good practice to take a look at the dimensions and rows/columns of all these dataframes to ensure that they were imported correctly. 

(Omitting the calls to glimpse/view.)

A few problems that stand out when glimpsing the data: 

* Two of the USDA tables, emp_df and pov_df, are way too narrow. The tidy philosophy of data requires that different variables (such as the number of employed people and the number of unemployed people) should always occupy different columns. This table has too few columns, so that the "Value" column contains the values of many different variables. Fortunately we have just the necessary tools to "pivot out" these overly packed columns. 

* The second one is also fairly minor. A 5-digit FIPS (Federal Information Processing Standard) code is assigned to every county-level political entity in the united states, including independent county-cities and the like. The first two digits of the code indicate the state and the last three indicate the county within the state. For some states the two-digit FIPS prefix begins with 0: for example, the FIPS prefix for counties in Alabama is 01. Thus the FIPS code should be recorded as a character string, not as a numeric value, which will typically shave off any superfluous leading 0's.

Because the FIPS code uniquely identifies a county in all government records, it's the correct choice of key column for joining data frames together.

Whoever prepared the data imported into emp_df, pov_df, pop_df, and density_df failed to take this into account, and we will have to "prepend" the FIPS fields for each of these tibbles with 0 (casting the values to character in the process). 

Also two of the other tables (vote_df and educ_df) have different names for the FIPS, which we mutate using dplyr into "fips". 

Note: after we join all the dataframes together, we only select a (relatively) small number of total columns from the huge composite tibble. This could have been avoided by selecting the columns earlier, and it's possible that in a setting where computational speed is essential, selecting first could have been preferable. 

# WRANGLING DATA

In this section we carry out all the changes indicated in the "Inspecting Data" section. 

Widen the emp_df and pov_df tables:

```{r pivots}
pov_df <- pov_df %>%
    pivot_wider(names_from = Attribute,
                values_from = Value) 
emp_df <- emp_df %>%
    pivot_wider(names_from = Attribute,
                values_from = Value)
```


Create zero_prepending function to apply to incorrectly coded FIPS:

```{r prepend_zero}
prepend_zero <- function(number_value){
    char_value <- as.character(number_value)
    if(str_length(char_value) < 5){
        char_value <- paste("0",char_value, sep = "")
    }
    char_value
}
prepend_zero <- Vectorize(prepend_zero)
```


Prepend zeros (also mutate nonstandard FIPS column names)

```{r fips_mutates}

density_df <- density_df %>%
    mutate(fips = prepend_zero(fips))

pov_df <- pov_df %>%
    mutate(fips = prepend_zero(FIPStxt))

emp_df <- emp_df %>%
    mutate(fips = prepend_zero(fips_txt))

pop_df <- pop_df %>%
    mutate(fips = prepend_zero(FIPStxt))

vote_df <- vote_df %>%
    rename(fips = geoid)

educ_df <- educ_df %>%
    rename(fips = 'FIPS Code')

```

Unmentioned in previous section but we also add in dem_pct and gop_pct to the vote_df

```{r pol_pcts}

vote_df <- vote_df %>%
    mutate(
           dem_pct = 100*votes_dem/(votes_dem+votes_gop),
           gop_pct = 100*votes_gop/(votes_dem+votes_gop)
          )

```

Add in categorical variable (did the county vote Dem/GOP in 2020 Pres) to vote_df.


```{r majority_vote}

majority_vote <- function(votes_dem, votes_gop){
    vote = 'D'
    if(votes_dem < votes_gop){
        vote = 'R'
    }
    vote
}
majority_vote <- Vectorize(majority_vote)

```

(Whenever you're using a function in mutate, don't forget to vectorize!)

Create majority vote.

```{r majority_vote_mutate}

vote_df <- vote_df %>%
    mutate(majority = majority_vote(votes_dem, votes_gop))
    
vote_df$majority <- as.factor(vote_df$majority)

```

We also include a state-level majority to decide how many states we classify incorrectly.

```{r state_majorities}

state_df <- vote_df %>%
    group_by(t_state_name) %>%
    summarise(total_dem = sum(votes_dem), 
              total_gop = sum(votes_gop)) %>%
    mutate(state_majority = majority_vote(total_dem,
                                          total_gop))

state_df$state_majority <- as.factor(state_df$state_majority)

vote_df <- vote_df %>%
    inner_join(state_df, by = 't_state_name')

```


# JOINING + SELECTING DATA

The order matters, because the helper function we will use to create maps was tailored to the table in census_df, so we want to make sure the counties come out in the order specified by census_df. 

```{r main_join}

df <- census_df %>%
    left_join(density_df, by = "fips") %>%
    left_join(educ_df, by = "fips") %>%
    left_join(emp_df, by = "fips") %>%
    left_join(pop_df, by = "fips") %>%
    left_join(pov_df, by = "fips") %>%
    left_join(vote_df, by = "fips")

```


We need to choose variables of interest. There are hundreds of variables in df currently, far more than we need. We'll just choose a few that, based on popular discourse, seem likely to
influence the election outcome of a county. 

* From census_df we'll take: 

  + total.pop
  + white
  + black
  + hispanic
  + asian

* From density_df we'll just take:

  + Population density (ppl/sq mi.) which is labeled as POP060210

* From educ_df we will take the (fairly self-explanatory) variables: 

  + Percent of adults with less than a high school diploma, 2015-19
  + Percent of adults with a high school diploma only, 2015-19
  + Percent of adults completing some college or associate's degree, 2015-19
  + Percent of adults with a bachelor's degree or higher, 2015-19

* From emp_df we will take the variables 

  + Unemployment_rate_2019
  + Median_Household_Income_2019
  + Med_HH_Income_Percent_of_State_Total_2019
  + Rural_urban_continuum_code_2013
  + Urban_influence_code_2013  
  + Metro_2013

  + The first three of these are fairly self-explanatory ('HH'=Household).

  + The last three are ordinal variables that describe how rural/urban a county is. 

  + The documentation for these is available for download along with the data sets at https://www.ers.usda.gov/data-products/county-level-data-sets/download-data/

  + For example, the Urban Influence code assigns values on a 1-12 scale
		
  + * 1	In large metro area of 1+ million residents			
    * 2	In small metro area of less than 1 million     residents			
    * 11	Noncore not adjacent to metro or micro area and contains a town of at least 2,500 residents			
    * 12	Noncore not adjacent to metro or micro area and does not contain a town of at least 2,500 residents			

* The population statistics dataset is particularly large, 166 variables. We only take: 

  + POP_ESTIMATE_2019
  + N_POP_CHG_2019 (population change in the year 2019)
  + R_NATURAL_INC_2019 (natural increase = birth rate-death rate)
  + INTERNATIONAL_MIG_2019 (international migration to this FIPS)

* The povery dataset has only 34 variables, we'll take: 

  + PCTPOVALL_2019 (pct of population in poverty in 2019)
  + POV017_2019	Estimate of people age 0-17 in poverty 2019

* The vote dataset only contributes the variable majority.

```{r remedy001}

good_columns <- c( 
                  "fips",
                  "total_pop",
                  "white",
                  "black",
                  "hispanic",
                  "asian",
                  "Percent of adults with less than a high school diploma, 2015-19", 
                  "Percent of adults with a high school diploma only, 2015-19",
                  "Percent of adults completing some college or associate's degree, 2015-19",
                  "Percent of adults with a bachelor's degree or higher, 2015-19",
                  "Unemployment_rate_2019",
                  "Median_Household_Income_2019",
                  "Med_HH_Income_Percent_of_State_Total_2019",
                  "Rural_urban_continuum_code_2013",
                  "Urban_influence_code_2013",
                  "Metro_2013",
                  "POP_ESTIMATE_2019",
                  "N_POP_CHG_2019",
                  "R_NATURAL_INC_2019",
                  "INTERNATIONAL_MIG_2019",
                  "PCTPOVALL_2019",
                  "POV017_2019",
                  "POP060210",
                  "majority")

main_df <- df %>%
    select(all_of(good_columns))

```

Rename a bunch of columns

```{r rename_cols}

main_df <- main_df %>%
    rename("no_high_sch" = "Percent of adults with less than a high school diploma, 2015-19",
           "high_sch%" = "Percent of adults with a high school diploma only, 2015-19",
           "some_col%" = "Percent of adults completing some college or associate's degree, 2015-19",
           "college%" = "Percent of adults with a bachelor's degree or higher, 2015-19",
           "unemployment" = "Unemployment_rate_2019",
           "med_hh_income" = "Median_Household_Income_2019",
           "med_hh_income_pct_state" = "Med_HH_Income_Percent_of_State_Total_2019",
           "rural_urban_continuum" = "Rural_urban_continuum_code_2013",
           "urban_influence" = "Urban_influence_code_2013",
           "metro_code" = "Metro_2013",
           "population" = "POP_ESTIMATE_2019",
           "pop_change" = "N_POP_CHG_2019",
           "nat_incr_rate" = "R_NATURAL_INC_2019",
           "intl_migration" = "INTERNATIONAL_MIG_2019",
           "poverty_pct" = "PCTPOVALL_2019",
           "child_poverty" = "POV017_2019",
           "ppl_per_sq_mi" = "POP060210"
           )

```


# CALIBRATING DECISION TREES

As always we need to split main_df into a training set and a test set.

One way to do this is to create an id column that we can use an antijoin on. 

```{r id_col}

main_df <- main_df %>% 
    mutate(id = row_number())

```


Optionally, we can check the ids 
<!-- head(main_df$id) -->

Make training/test sets.
```{r train_test_split}
main_df <- main_df[ , !(colnames(main_df) == "fips")]
train <- main_df %>% sample_frac(.70)
test  <- anti_join(main_df, train, by = 'id')
train <- train[, !(colnames(train) == "id")]
train <- test[, !(colnames(train) == "id")]
```

R's decision tree library complains a lot if there is a lot of missing data. We take the lazy, suboptimal route and just omit the data if something is missing.
We want to predict outcome (i.e. majority party) with the freedom to use every variable except for fips (too specific). 

We use the rpart function instead of tree (better plotting, flexibility, etc.). 
In our Shiny app, control parameters for this rpart call come from user input. 

```{r tree_train}


tree_majority <- rpart(majority~.,
                       method = "class", 
                       data = na.omit(train),
                       cp = 0.003)
```

Now we can use the prp function to plot it. 

```{r plot_tree}
prp(tree_majority, 
    uniform=TRUE,
    main="Which way does a county vote?", 
    Margin = 0.05,
    border.col = "black",
    split.cex = 0.5, 
    extra = 0)
```


Interpreting the decision tree is fairly easy: the first and most important question when deciding how a county will vote is the percentage of residents who have a college degree (or more). The counties in which at least 32 percent of residents have a college degree are very likely to vote Democratic. However, such a county can still vote Republican if it has relatively few Asian residents and is populated by high earners (the county's median household income is 121 percent of the state median).

On the other hand, a county with few (< 32 percent) of residents having college degrees can vote Democratic as long as POC make up a substantial majority of the population (white < 42 percent). 
None of these ideas are tremendously surprising, especially for anyone who followed post-election demographic analysis published newspapers and other media after the 2020 election. If we reduce the complexity parameter (cp) in the call to rpart, we can generate far more complicated trees, with several dozen nodes. These will outperform our somewhat modestly sized tree on the training set, but they will suffer from heavy overfitting and their test accuracy will be worse. They will also offer somewhat more interesting conclusions about the variables, involving some of the variables (like poverty rates, population density, )

The following exports a nice .png of the decision tree model. 

```{r export_graphic}
post(tree_majority, file = "decision_tree.png",
     title = "CountyVotingTree")
```


If we view the summary (call summary(tree_majority)) or look at we see the variables used are college_or_more (this is the % of the county population that finished college/university), intl_immigration, as well as white and black population percentages. 


```{r remedy002}

test_pred <- predict(tree_majority, test, type = "class")

```


Now lets use the caret package to produce a nice confusion matrix: 

```{r}
confusionMatrix(test_pred, test$majority
                )
```

Our model has one drawback: when viewing the Democratic class as the "positive" result, the sensitivity (aka recall) of the tree model is only 74 percent. A quarter of all Democratic counties are incorrectly classified as Republican. On the other hand the specificity (aka precision) is 95%, so that only one in 20 Republican counties is incorrectly classified as Democratic. 

The complexity parameter (cp) in the call to rpart controls how deep the tree is grown. 

```{r}
parameter_grid <- 0.0001*(1:5000)

precision_recall_fn <- function(parameters, 
                                dataset = train, 
                                testdata = test, 
                                testval = test$majority
                                ){
  precision_vec <- numeric(length(parameters))
  recall_vec <- numeric(length(parameters))
  
  for(i in 1:length(parameters)){
    new_model <- rpart(majority~.,
                       method = "class", 
                       data = na.omit(dataset),
                       cp = parameters[i])
    test_pred <- predict(new_model, testdata, type = "class")
    new_matrix <- confusionMatrix(test_pred, testval
                )
    recall_vec[i] <- new_matrix$byClass[1]
    precision_vec[i] <- new_matrix$byClass[2]
  }
  output <- list(precision_vec, recall_vec, parameters)
}

results <- precision_recall_fn(parameter_grid)
```


Now we can plot these 

```{r}
plot(xvec, rvec, main = "Recall vs. complexity", xlab = "Complexity", ylab = "Recall")
plot(xvec, pvec, main = "Precision vs. complexity", xlab = "Complexity", ylab = "Precision")
```

It's not surprising that these would be somewhat in conflict with one another -- in vague terms low complexity models favor the majority (Republican counties) and high complexity models favor the minority (Democratic counties). It looks like our initial guess of cp = 0.03 is a very good complexity parameter was actually quite fortuitous. (Some rough experiments led to this choice, so it wasn't a complete shot in the dark.)